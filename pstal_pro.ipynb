{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ce02073",
   "metadata": {},
   "source": [
    "## 1. Initial Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711721df",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "import torch\n",
    "print(f\"CUDA disponible: {torch.cuda.is_available()}\")\n",
    "print(f\"Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e72f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Créer un dossier de travail dans Drive\n",
    "# !mkdir -p /content/drive/MyDrive/pstal_project\n",
    "%cd /content/drive/MyDrive/\n",
    "\n",
    "# CHOISIR L'UNE DES OPTIONS CI-DESSOUS:\n",
    "\n",
    "# Option GitHub (public) - Remplacer par votre URL\n",
    "!git clone https://github.com/brahimharounhassan/pstal_project.git\n",
    "\n",
    "\n",
    "%cd pstal_project\n",
    "!git status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5909ed",
   "metadata": {},
   "source": [
    "## 3. Installer les dépendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5565fdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets peft optuna torch torchvision torchaudio sentencepiece protobuf\n",
    "!pip list | grep -E 'transformers|peft|torch'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afa76db",
   "metadata": {},
   "source": [
    "## 4. Vérifier la structure du projet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d0e670",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la\n",
    "!ls -la data/sequoia/\n",
    "!ls -la src/tp3/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7768caa",
   "metadata": {},
   "source": [
    "## 5. Créer les dossiers nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af96514c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p models checkpoints predictions logs outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62c8a68",
   "metadata": {},
   "source": [
    "## 6. OPTION A - Recherche d'hyperparamètres avec Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d9c676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Réduire le nombre de trials pour Colab (gratuit = 2h max)\n",
    "# Modifier configs/config.yml: n_trials: 10 au lieu de 30\n",
    "# Modifier configs/config.yml: n_epochs: 10 au lieu de 15\n",
    "\n",
    "!python src/tp3/hp_tuning.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b8cb11",
   "metadata": {},
   "source": [
    "## 6. OPTION B - Utiliser les hyperparamètres optimaux directement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3981b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Meilleurs hyperparamètres trouvés (basés sur vos expériences)\n",
    "best_hp = {\n",
    "    \"r\": 32,\n",
    "    \"lora_alpha\": 64,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"lr\": 2e-4,\n",
    "    \"bs\": 16,\n",
    "    \"use_dora\": True,\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"weight_decay\": 1e-4\n",
    "}\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "with open(f\"outputs/best_hyperparameters_{timestamp}.json\", \"w\") as f:\n",
    "    json.dump(best_hp, f, indent=2)\n",
    "\n",
    "print(\"Hyperparamètres sauvegardés\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20292732",
   "metadata": {},
   "source": [
    "## 7. Fine-tuning LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc2e76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/fine_tuning.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ea9c1e",
   "metadata": {},
   "source": [
    "## 8. Entraîner le classifieur super-sense (10-20 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec4be38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trouver le modèle fine-tuné\n",
    "import glob\n",
    "finetuned_models = glob.glob(\"models/final_model_*.pt\")\n",
    "latest_model = max(finetuned_models, key=lambda x: x.split('_')[-1])\n",
    "print(f\"Modèle fine-tuné: {latest_model}\")\n",
    "\n",
    "!python src/tp3/train_ssense.py --finetuned_model {latest_model} --n_epochs 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ff325a",
   "metadata": {},
   "source": [
    "## 9. Prédictions sur dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ebeb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Trouver le dernier checkpoint\n",
    "checkpoints = glob.glob(\"checkpoints/ssense_checkpoint_*.pt\")\n",
    "latest_checkpoint = max(checkpoints, key=lambda x: x.split('_')[-1])\n",
    "print(f\"Checkpoint: {latest_checkpoint}\")\n",
    "\n",
    "!python src/tp3/predict_ssense.py \\\n",
    "    --checkpoint {latest_checkpoint} \\\n",
    "    --input_file data/sequoia/sequoia-ud.parseme.frsemcor.simple.dev \\\n",
    "    --output_file predictions/ssense_colab_dev.conllu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d874fe31",
   "metadata": {},
   "source": [
    "## 10. Évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3252f15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python lib/evaluate.py \\\n",
    "    --pred predictions/ssense_colab_dev.conllu \\\n",
    "    --gold data/sequoia/sequoia-ud.parseme.frsemcor.simple.dev \\\n",
    "    --tagcolumn frsemcor:noun \\\n",
    "    --train data/sequoia/sequoia-ud.parseme.frsemcor.simple.train \\\n",
    "    --upos-filter NOUN PROPN NUM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e017559d",
   "metadata": {},
   "source": [
    "## 11. Télécharger les résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ced91a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "# Créer un zip avec tous les résultats\n",
    "!zip -r results.zip models/ checkpoints/ predictions/ logs/ outputs/\n",
    "\n",
    "# Télécharger\n",
    "files.download('results.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f814f033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurer Git (à faire une seule fois)\n",
    "!git config --global user.email \"votre.email@example.com\"\n",
    "!git config --global user.name \"Votre Nom\"\n",
    "\n",
    "# Ajouter et commiter les résultats\n",
    "!git add outputs/*.json\n",
    "!git add predictions/*.conllu\n",
    "!git add logs/*.log\n",
    "\n",
    "!git commit -m \"Training results from Colab - $(date +%Y-%m-%d)\"\n",
    "\n",
    "# Pousser vers GitHub/GitLab\n",
    "!git push origin main\n",
    "\n",
    "# Alternative: Télécharger un ZIP\n",
    "from google.colab import files\n",
    "!zip -r results_$(date +%Y%m%d_%H%M%S).zip models/ predictions/ logs/ outputs/ -x \"*.pt\"\n",
    "# files.download('results_*.zip')  # Décommenter pour télécharger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92ebf69",
   "metadata": {},
   "source": [
    "## 12. Sauvegarder et pousser les résultats sur Git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409fcb0a",
   "metadata": {},
   "source": [
    "## CONSEILS POUR COLAB GRATUIT\n",
    "\n",
    "### Limites\n",
    "- **12h max de session** puis déconnexion\n",
    "- **GPU T4 gratuit** : ~16GB VRAM (suffisant pour CamemBERT-base)\n",
    "- Session peut être interrompue si inactivité\n",
    "\n",
    "### Optimisations\n",
    "1. **Réduire n_trials à 10-15** au lieu de 30 (hp_tuning.py)\n",
    "2. **Réduire n_epochs à 10-15** pour l'Optuna search\n",
    "3. **Garder n_epochs=30-50** pour le training final\n",
    "4. **Batch size 16** fonctionne bien sur T4\n",
    "5. **Sauvegarder sur Drive** pour ne pas perdre les résultats\n",
    "\n",
    "### Pipeline complet\n",
    "```python\n",
    "# 1. Optuna search (2-4h) - OPTIONNEL si vous avez déjà les HP\n",
    "!python src/tp3/hp_tuning.py\n",
    "\n",
    "# 2. Fine-tuning LoRA (30-60 min)\n",
    "!python src/tp3/fine_tuning.py\n",
    "\n",
    "# 3. Train classifier (10-20 min)\n",
    "!python src/tp3/train_ssense.py --finetuned_model models/final_model_XXX.pt\n",
    "\n",
    "# 4. Predict\n",
    "!python src/tp3/predict_ssense.py --checkpoint checkpoints/ssense_checkpoint_XXX.pt\n",
    "\n",
    "# 5. Evaluate\n",
    "!python lib/evaluate.py --pred predictions/XXX.conllu --gold data/...\n",
    "```\n",
    "\n",
    "### Surveillance\n",
    "```python\n",
    "# Vérifier l'usage GPU en temps réel\n",
    "!watch -n 1 nvidia-smi\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
