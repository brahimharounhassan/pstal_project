\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{footnote}

\geometry{margin=2.5cm}
\onehalfspacing

\definecolor{codebg}{rgb}{0.95,0.95,0.95}
\lstset{
    backgroundcolor=\color{codebg},
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    rulecolor=\color{gray!30},
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red},
    showstringspaces=false,
    tabsize=4,
    language=Python
}

\title{Étiqueteur super-senses avec transformers pré-entraînés\\
\large PSTAL (Prédiction structurée pour le traitement automatique de la langue)\\
\large TP 3}
\date{}

\begin{document}

\maketitle

L’objectif de ce TP est de développer et évaluer un système capable de prédire des étiquettes sémantiques appelées \textbf{super-senses} (colonne \texttt{frsemcor:noun} des nos données \texttt{sequoia} au format CoNLL-U). Ces étiquettes indiquent, pour chaque nom, sa catégorie sémantique gros-grain, par exemple : \textit{surprise} est un \textit{sentiment} (étiquette \texttt{Feeling}), \textit{jambe} est une \textit{partie du corps} (\texttt{Body}), \textit{avocat} est une \textit{personne} (\texttt{Person}) ou un \textit{aliment} (\texttt{Food}) selon le contexte, etc. Nous utiliserons des représentations vectorielles appelées \textit{embeddings contextuels}, issues de modèles transformers pré-entraînés.

\section{Étiquetage en super-senses}
\label{sec1}

L’étiquetage du sens des mots est une tâche historiquement difficile en TAL\footnote{Cette tâche est aussi appelée désambiguisation lexicale ou WSD, de l’anglais \textit{word sense disambiguation}.}. Les premiers modèles, peu performants, essayaient de prédire, parmi tous les sens listés dans un dictionnaire, le plus approprié dans le contexte. \href{https://aclanthology.org/W03-1022/}{Ciaramita \& Johnson (2003)} ont été les premiers à proposer de simplifier cette tâche à l’aide de catégories plus génériques, car la granularité trop fine des dictionnaires a longtemps été une difficulté en WSD.

Les données que nous utiliserons sont issues du projet \textbf{FR-SemCor}, qui a annoté le corpus Sequoia en super-senses uniquement pour les \underline{noms} \href{https://aclanthology.org/2020.lrec-1.724/}{(Barque et al. 2020)}\footnote{Nous utilisons une version simplifiée, voir \texttt{data/sequoia/README.md} pour les détails.}. Donc, les mots qui nous intéressent sont les noms communs, les noms propres et les numéros ; ceux dont la \texttt{upos} $\in \{\texttt{NOUN}, \texttt{PROPN}, \texttt{NUM}\}$ (\texttt{upos} est la colonne 4 des nos données \texttt{sequoia} au format CoNLL-U). Le système doit prédire une des 24 étiquettes super-sense nominales possibles, ou alors prédire \texttt{*} pour les mots n’ayant pas de super-sense annoté\footnote{L’étiquette \texttt{*} est prédite dans 2 situations : soit le mot n’est pas un nom, soit il l’est, mais son annotation est manquante.}.

Il s’agit, comme les TP précédents (accessible dans les sous-répertoires \texttt{src/tp1/} et \texttt{src/tp2/}), d’une tâche d’étiquetage de séquences, par exemple :

\begin{center}
\begin{tabular}{ccccccccccccc}
& $w_1$ & $w_2$ & $w_3$ & $w_4$ & $w_5$ & $w_6$ & $w_7$ & $w_8$ & $w_9$ & $w_{10}$ & $w_{11}$ & $w_{12}$ \\
$x =$ &\textit{Quelle} & \textit{surprise} & \textit{!} & \textit{Arturo} & \textit{arrive} & \textit{en} & \textit{Chine} & \textit{au} & \textit{moment} & \textit{de} & \textit{la} & \textit{fête} \\
& $\Downarrow$ & $\Downarrow$ & $\Downarrow$ & $\Downarrow$ & $\Downarrow$ & $\Downarrow$ & $\Downarrow$ & $\Downarrow$ & $\Downarrow$ & $\Downarrow$ & $\Downarrow$ & $\Downarrow$ \\
$y =$ & \texttt{*} & \texttt{Feeling} & \texttt{*} & \texttt{Person} & \texttt{*} & \texttt{*} & \texttt{Institution} & \texttt{*} & \texttt{Time} & \texttt{*} & \texttt{*} & \texttt{Act}\\
& \textit{t1} & \textit{t2} & \textit{t3} & \textit{t4} & \textit{t5} & \textit{t6} & \textit{t7} & \textit{t8} &
\textit{t9} & \textit{t10} &
\textit{t11} & \textit{t12} 

\end{tabular}
\end{center}

\section{Bibliothèque \texttt{transformers}}
\label{sec2}

Le système repose sur des \textit{embeddings} extraits d’un modèle de langage \textit{transformer} bidirectionnel du type « BERT ». Ces embeddings sont optimisés pour prédire un mot en fonction de son contexte (modèle de langage masqué, ou MLM). Comme le contexte et le sens sont corrélés selon l’hypothèse distributionnelle, nous supposons qu’il est pertinent de prédire les \textit{super-senses} à partir de ces embeddings contextuels.

Généralement, on ignore la couche MLM, et on garde le « mille-feuilles » en dessous. Une nouvelle phrase à encoder passe dans les $N$ couches d’auto-attention et, à la dernière (\texttt{last\_hidden\_state}), on obtient un embedding $d$-dimensionnel pour chaque token de l’entrée. Sur ces embeddings, on ajoute une couche de décision (p. ex. linéaire) spécifique à notre tâche, qui sera apprise. On parle de \textbf{fine-tuning} quand l’apprentissage de la couche de décision rétro-propage les gradients vers l’intérieur du \textit{transformer} et modifie les embeddings. Nous utiliserons les embeddings du \textit{transformer} de manière \textbf{statique} : on parle alors d’\textit{embeddings contextuels}.

La bibliothèque \textit{transformers} de HuggingFace contient des modules pratiques pour ce TP. De plus, des centaines de modèles pré-entraînés sont disponibles sur \url{https://huggingface.co/models}. Ces modèles contiennent l’architecture du réseau et les valeurs de tous les paramètres : embeddings, matrices d’auto-attention, couches intermédiaires, etc. De plus, les modèles se téléchargent automatiquement dès la première utilisation !

Les modèles ont été pré-entraînés sur des corpus dans une ou plusieurs langues. Certains sont uniquement francophones, comme \texttt{almanach/camembert-*} et \texttt{flaubert/flaubert\_*}, alors que d’autres modèles tels que \texttt{google-bert/bert-base-multilingual-cased} et \texttt{distilbert/distilbert-base-multilingual-cased} incluent le français parmi une centaine de langues vues à l’entraînement. Pour le développement, \texttt{distilbert} est préférable par sa relative légèreté. Utilisez \texttt{camembert} et \texttt{google-bert} quand le développement sera fini.

\section{Préparation des données}
\label{sec3}

Chaque modèle de langage a son propre tokéniseur, p. ex. \texttt{CamembertTokenizer}. Pour instancier le tokéniseur à partir du nom du modèle, on utilise \texttt{AutoTokenizer.from\_pretrained}. La phrase à tokéniser est une liste de mots avec \texttt{is\_split\_into\_words=True}. Pour obtenir un tenseur en sortie, \texttt{return\_tensors='pt'} \footnote{Avec \texttt{convert\_ids\_to\_tokens}  \texttt{['<s>', '\_Art', 'uro', '\_devra', '\_re', 'tent', 'er', '\_demain', '</s>']}.} :

\begin{lstlisting}[language=Python]
>>> from transformers import AutoTokenizer, AutoModel
>>> tokenize = AutoTokenizer.from_pretrained("almanach/camembert-base")
>>> tok_sent = tokenize(["Arturo","devra","retenter","demain"],
...                     is_split_into_words=True, return_tensors='pt')
>>> print(tok_sent['input_ids'])
tensor([[   5, 2082, 10394, 2613, 343, 3889, 108, 2385,    6]])
\end{lstlisting}

La sortie du tokéniseur peut être donnée au modèle, qui générera un embedding ($d = 768$) par token :

\begin{lstlisting}[language=Python]
>>> model = AutoModel.from_pretrained("almanach/camembert-base")
>>> emb_sent = model(**tok_sent)['last_hidden_state'][0]
>>> print(emb_sent.shape)
torch.Size([9, 768])
\end{lstlisting}

\textbf{Alignement (sub-)tokens $\leftrightarrow$ mots}. Notez que, pour 4 mots, nous obtenons 9 embeddings, correspondant aux 9 \textit{sub-tokens} générés par le tokéniseur. Cependant, les annotations de \textit{super-sense} sont alignées aux mots, et non pas aux sub-tokens. Par exemple, nous ne pourrions pas créer un \texttt{DataLoader} avec 9 entrées correspondant à 4 sorties : il faut les aligner ! La fonction \texttt{tok\_sent.word\_ids()} permet d’obtenir l’alignement entre les mots et les sub-tokens\footnote{Cette fonction n’est pas disponible pour \texttt{flaubert}, raison pour laquelle ce modèle est déconseillé.} :

\begin{lstlisting}[language=Python]
>>> tok_sent.word_ids()
[None, 0, 0, 1, 2, 2, 2, 3, None]
\end{lstlisting}

Par exemple, le mot d’indice 2 contient 3 sub-tokens, dans les positions 4, 5 et 6 de \texttt{tok\_sent['input\_ids']}.

Votre fonction d’alignement doit donc parcourir les mots de la phrase et, pour chaque mot ayant le \texttt{upos} $\in \{\texttt{NOUN}, \texttt{PROPN}, \texttt{NUM}\}$, trouver le(s) embedding(s) correspondant(s) à l’aide de \texttt{word\_ids}. Créez ensuite un seul embedding pour ce mot en moyennant les embeddings contextuels de ses sub-tokens. Cet embedding sera associé à l’étiquette super-sense du mot. Figez les embeddings avec \texttt{torch.no\_grad()} lors de l’appel du modèle.

Pensez à transformer en indices les étiquettes de super-sense, car \texttt{transformers} ne le fait pas pour nous.

\section{Classifieur d’embeddings contextuels}
\label{sec4}

Si la préparation des données est complexe et laborieuse, le classifieur, lui, est assez facile à programmer. Il s’agit d’un MLP composé de une ou plusieurs couches linéaires (\texttt{nn.Linear}) suivies de fonctions d’activation non-linéaires (p. ex. \texttt{nn.ReLU}). La première couche dense prend en entrée l’embedding contextuel du mot à classifier. Sa dimension dépend du modèle pré-entraîné utilisé (\texttt{camembert}, \texttt{distilbert}, etc.) et peut être obtenue avec \texttt{config.hidden\_size}. La dernière couche aura comme fonction d’activation le softmax (implicite, comme d’habitude). Sa dimension est égale au nombre d’étiquettes de super-sense à prédire, plus 1 position correspondant à la prédiction de \texttt{*}. Il n’est pas nécessaire de faire du padding comme on ferait dans RNN.

\section{Entraînement, prédiction et évaluation}
\label{sec5}

L’entraînement requiert un \texttt{DataLoader} associant embeddings contextuels et étiquettes super-sense. À chaque epoch, calculez la loss et l’accuracy sur le \texttt{.dev}. Comme nous n’avons pas de padding, il n’y a pas besoin de masquage. Sauvegardez le modèle, le vocabulaire des étiquettes et les hyper-paramètres avec \texttt{torch.save}.

Le nom du modèle pré-entraîné est un hyper-paramètre important à sauvegarder. Pour la prédiction, mettez \texttt{*} dans la colonne \texttt{frsemcor:noun} de tous les mots n’ayant pas une des 3 POS cible. Pour les autres, obtenez les embeddings contextuels alignés, les passez dans le classifieur et choisissez l’étiquette de score maximal.

Pour évaluer les prédictions, utilisez le script \texttt{lib/evaluate.py}, déjà utilisé lors des TP précédents (dans les sous-répertoires \texttt{src/tp1/} et \texttt{src/tp2/}), avec les options :

\begin{itemize}[leftmargin=*]
    \item \texttt{-- tagcolumn frsemcor:noun} : indique la colonne prédite (12\up{ème}), qui s’appelle \texttt{frsemcor:noun}.
    \item \texttt{--upos-filter NOUN PROPN NUM} : nous ne voulons pas considérer les \texttt{*} prédites sur les mots n’ayant pas les POS cible (déterminants, verbes, etc.). Cela gonflerait artificiellement le score d’évaluation. Cette option permet de calculer l’\textit{accuracy} uniquement pour les mots dont la POS nous intéresse.
    \item \texttt{--train} : permet de calculer l’\textit{accuracy} sur les OOV, ce qui peut être utile pour comparer des modèles.
\end{itemize}

\section{Travail à effectuer}
\label{sec6}

Vous devez écrire deux scripts : \texttt{train\_ssense.py} pour l’entraînement du modèle, et \texttt{predict\_ssense.py} pour la prédiction des \textit{super-senses}. Le code du classifieur (Section~\ref{sec4}) doit être partagé par les deux scripts.

L’évaluation des prédictions sera effectuée par le script fourni \texttt{lib/evaluate.py} en utilisant les options \texttt{--upos-filter NOUN PROPN NUM} et \texttt{--tagcolumn frsemcor:noun}, comme expliqué en Section~\ref{sec5}. Nous suggérons de suivre les étapes ci-dessous :

\begin{enumerate}[leftmargin=*]
    \item Se familiariser avec le tokéniseur et le modèle Camembert de la bibliothèque \texttt{transformers} \\
    {\small $\rightarrow$ Exécuter les exemples ci-dessus, ainsi que ceux sur les slides du cours (dans le sous-répertoire \texttt{cm-code/}).}
    
    \item Partir du code du TP 1 (accessible dans le sous-répertoire \texttt{src/tp1/} du répertoire courant \texttt{pstal-etu/}), et modifier la colonne à prédire (\texttt{frsemcor:noun}), tester et évaluer.

    \item Nous utiliserons \texttt{distilbert/distilbert-base-multilingual-cased} dans un premier moment. Ce modèle n’est pas bon pour le français, mais il est léger et permet de faire tester le code rapidement.
    Écrire la fonction de préparation des données qui doit :
    \begin{enumerate}[label=(\alph*)]
        \item Tokéniser chaque phrase avec le tokéniseur Camembert.
        \item Passer chaque phrase tokénisée dans le modèle et récupérer les embeddings contextuels en sortie.
        \item Pour chaque mot ayant une POS concernée, moyenner les embeddings de ses tokens, et l’associer à l’étiquette \textit{super-sense} correspondante.
        \item Mettre les paires embedding–\textit{super-sense} dans un tenseur puis dans un \texttt{DataLoader}.
    \end{enumerate}
    
    \item Écrire le classifieur linéaire qui prend en entrée les embeddings contextuels et prédit le \textit{super-sense}.

    {\small $\rightarrow$ Classifieur simple : entrée = vecteur de taille fixe (embedding moyen des tokens du mot), sortie = étiquette \textit{supersense}}

    
    \item Adapter les fonctions \texttt{fit} et \texttt{perf} à ce classifieur, tester, évaluer.

    \item Modifier le modèle pour utiliser \texttt{almanach/camembert-base} à la place de \textbf{DistilBERT}, observer l’amélioration des scores d’évaluation

    \item Tester aussi \texttt{almanach/camembert-large} et \texttt{google-bert/bert-base-multilingual-cased}, observer les différences de performance
\end{enumerate}

\end{document}